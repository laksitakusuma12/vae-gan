{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.framework.ops import disable_eager_execution\n",
    "from sklearn.model_selection import train_test_split\n",
    "disable_eager_execution()\n",
    "import nltk\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('D:/tesis/scisumm/scisumm.csv')\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize per sent\n",
    "from nltk.tokenize import sent_tokenize\n",
    "sent_for_print = [sent_tokenize(s.lower()) for s in df['text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['text']\n",
    "y = df['summary']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0, train_size = .75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.str.lower()\n",
    "X_test  = X_test.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = nltk.RegexpTokenizer(r\"\\w+\")\n",
    "train   = [tokenizer.tokenize(s.lower()) for s in X_train]\n",
    "test    = [tokenizer.tokenize(s.lower()) for s in X_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\LENOVO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Stopword Removal\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "listStopword = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Menghapus kata-kata di stopwords\n",
    "train_stopwords = []\n",
    "for sent in train:\n",
    "  filter = [s for s in sent if s not in listStopword]\n",
    "  train_stopwords.append(filter)\n",
    "# train_stopwords\n",
    "\n",
    "test_stopwords = []\n",
    "for sent in test:\n",
    "  filter = [s for s in sent if s not in listStopword]\n",
    "  test_stopwords.append(filter)\n",
    "# test_stopwords\n",
    "\n",
    "# output merupakan sisa kata yg sudah dihapus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train   = [' '.join(t) for t in train_stopwords]\n",
    "test    = [' '.join(t) for t in test_stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing modules\n",
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "\n",
    "train   = [ps.stem(w) for w in train]\n",
    "test    = [ps.stem(w) for w in test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vec = TfidfVectorizer()\n",
    "\n",
    "train   = vec.fit_transform(train)\n",
    "test    = vec.transform(test)\n",
    "\n",
    "train   = train.toarray()\n",
    "test    = test.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " encoder_input (InputLayer)  [(None, 60704)]           0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1000)              60705000  \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 500)               500500    \n",
      "                                                                 \n",
      " z_mean (Dense)              (None, 250)               125250    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 61,330,750\n",
      "Trainable params: 61,330,750\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.layers import Input, Dense, Lambda\n",
    "from keras import backend as K\n",
    "from keras import Model\n",
    "\n",
    "# Setup the network parameters:\n",
    "original_dim = train.shape[1]\n",
    "input_shape = (original_dim, )\n",
    "# intermediate_dim = 200\n",
    "batch_size  = 16\n",
    "latent_dim  = 250\n",
    "epochs      = 10\n",
    "\n",
    "# Map inputs to the latent distribution parameters:\n",
    "# VAE model = encoder + decoder\n",
    "# build encoder model\n",
    "inputs      = Input(shape=input_shape, name='encoder_input')\n",
    "# x           = Dense(intermediate_dim, activation='relu')(inputs)\n",
    "x           = Dense(1000, activation='relu')(inputs)\n",
    "x           = Dense(500, activation='relu')(x)\n",
    "z_mean      = Dense(latent_dim, name='z_mean')(x)\n",
    "z_log_var   = Dense(latent_dim, name='z_log_var')(x)\n",
    "\n",
    "# Use those parameters to sample new points from the latent space:\n",
    "# reparameterization trick\n",
    "# instead of sampling from Q(z|X), sample epsilon = N(0,I)\n",
    "# z = z_mean + sqrt(var) * epsilon\n",
    "def sampling(args):\n",
    "    \"\"\"Reparameterization trick by sampling from an isotropic unit Gaussian.\n",
    "    # Arguments\n",
    "        args (tensor): mean and log of variance of Q(z|X)\n",
    "    # Returns\n",
    "        z (tensor): sampled latent vector\n",
    "    \"\"\"\n",
    "\n",
    "    z_mean, z_log_var = args\n",
    "    batch   = K.shape(z_mean)[0]\n",
    "    dim     = K.int_shape(z_mean)[1]\n",
    "    # by default, random_normal has mean = 0 and std = 1.0\n",
    "    epsilon = K.random_normal(shape=(batch, dim))\n",
    "    return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "# use reparameterization trick to push the sampling out as input\n",
    "# note that \"output_shape\" isn't necessary with the TensorFlow backend\n",
    "z = Lambda(sampling, output_shape=(latent_dim,), name='z')([z_mean, z_log_var])\n",
    "\n",
    "# Instantiate the encoder model:\n",
    "encoder = Model(inputs, z_mean, name='encoder')\n",
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " z_sampling (InputLayer)     [(None, 250)]             0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 500)               125500    \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1000)              501000    \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 60704)             60764704  \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 61,391,204\n",
      "Trainable params: 61,391,204\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Build the decoder model:\n",
    "latent_inputs   = Input(shape=(latent_dim,), name='z_sampling')\n",
    "# x               = Dense(intermediate_dim, activation='relu')(latent_inputs)\n",
    "x               = Dense(500, activation='relu')(latent_inputs)\n",
    "x               = Dense(1000, activation='relu')(x)\n",
    "outputs         = Dense(original_dim, activation='sigmoid')(x)\n",
    "\n",
    "# Instantiate the decoder model:\n",
    "decoder = Model(latent_inputs, outputs, name='decoder')\n",
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vae_mlp\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " encoder_input (InputLayer)  [(None, 60704)]           0         \n",
      "                                                                 \n",
      " encoder (Functional)        (None, 250)               61330750  \n",
      "                                                                 \n",
      " decoder (Functional)        (None, 60704)             61391204  \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 122,721,954\n",
      "Trainable params: 122,721,954\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the VAE model:\n",
    "outputs = decoder(encoder(inputs))\n",
    "vae     = Model(inputs, outputs, name='vae_mlp')\n",
    "vae.summary()\n",
    "\n",
    "# As in the Keras tutorial, we define a custom loss function:\n",
    "loss_object = tf.keras.losses.BinaryCrossentropy()\n",
    "def vae_loss(x, x_decoded_mean):\n",
    "    xent_loss   = loss_object(x, x_decoded_mean)\n",
    "    kl_loss     = - 0.5 * K.mean(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
    "    return xent_loss + kl_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 756 samples, validate on 253 samples\n",
      "Epoch 1/10\n",
      "756/756 [==============================] - ETA: 0s - loss: 0.0767"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\training_v1.py:2335: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates = self.state_updates\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "756/756 [==============================] - 73s 97ms/sample - loss: 0.0767 - val_loss: 0.0033\n",
      "Epoch 2/10\n",
      "756/756 [==============================] - 61s 80ms/sample - loss: 0.0028 - val_loss: 0.0021\n",
      "Epoch 3/10\n",
      "756/756 [==============================] - 60s 79ms/sample - loss: 0.0022 - val_loss: 0.0020\n",
      "Epoch 4/10\n",
      "756/756 [==============================] - 60s 79ms/sample - loss: 0.0021 - val_loss: 0.0019\n",
      "Epoch 5/10\n",
      "756/756 [==============================] - 60s 79ms/sample - loss: 0.0020 - val_loss: 0.0020\n",
      "Epoch 6/10\n",
      "756/756 [==============================] - 60s 79ms/sample - loss: 0.0020 - val_loss: 0.0019\n",
      "Epoch 7/10\n",
      "756/756 [==============================] - 60s 79ms/sample - loss: 0.0020 - val_loss: 0.0019\n",
      "Epoch 8/10\n",
      "756/756 [==============================] - 64s 85ms/sample - loss: 0.0020 - val_loss: 0.0019\n",
      "Epoch 9/10\n",
      "756/756 [==============================] - 61s 81ms/sample - loss: 0.0020 - val_loss: 0.0019\n",
      "Epoch 10/10\n",
      "756/756 [==============================] - 61s 81ms/sample - loss: 0.0020 - val_loss: 0.0019\n"
     ]
    }
   ],
   "source": [
    "# We compile the model:\n",
    "vae.compile(optimizer='rmsprop', loss=vae_loss)\n",
    "\n",
    "# Finally, we train the model:\n",
    "\n",
    "results = vae.fit(train, train,\n",
    "        shuffle=True,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        validation_data=(test, test)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\training_v1.py:2359: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Variational Autoencoder Encoder Reduced Scissum Dataset Size: :  (756, 250)\n",
      "\n",
      "\n",
      "Variational Autoencoder Scissum Dataset Size: :  (756, 60704)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_encoded = encoder.predict(train)\n",
    "reconstruction = vae.predict(train)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"Variational Autoencoder Encoder Reduced Scissum Dataset Size: : \", X_encoded.shape)\n",
    "print(\"\\n\")\n",
    "print(\"Variational Autoencoder Scissum Dataset Size: : \", reconstruction.shape)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(sent_1,sent_2):\n",
    "  s1_dot_s2=np.sum(np.multiply(sent_1,sent_2))\n",
    "  magnitude_of_s1=math.sqrt(np.sum(np.multiply(sent_1,sent_1)))\n",
    "  magnitude_of_s2=math.sqrt(np.sum(np.multiply(sent_2,sent_2)))\n",
    "  return s1_dot_s2/(magnitude_of_s1*magnitude_of_s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_matrix(reconstruction):\n",
    "  cosine_similarity_matrix=np.zeros(shape=(len(reconstruction),len(reconstruction)))\n",
    "  for i in range(len(reconstruction)):\n",
    "    for j in range(len(reconstruction)):\n",
    "      cosine_similarity_matrix[i][j]=cosine_similarity(reconstruction[i],reconstruction[j])\n",
    "  #normalize the matrix\n",
    "  for idx in range(len(cosine_similarity_matrix)):\n",
    "    cosine_similarity_matrix[idx] /= cosine_similarity_matrix[idx].sum()\n",
    "    \n",
    "  return cosine_similarity_matrix\n",
    "\n",
    "cosine_similarity_matrix=cosine_matrix(reconstruction)\n",
    "# print(cosine_similarity_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "756\n"
     ]
    }
   ],
   "source": [
    "def text_rank(cosine_matrix,eps=0.0001,d=0.85):\n",
    "  teleportation_factor=np.ones(len(reconstruction))/len(reconstruction)\n",
    "  i=0\n",
    "  while i<100:\n",
    "    new_p=np.dot(cosine_similarity_matrix.T,teleportation_factor.T)\n",
    "    if i<=100:\n",
    "      return new_p\n",
    "    i=i+1\n",
    "    teleportation_factor = new_p\n",
    "  \n",
    "  \n",
    "result_rank=text_rank(cosine_similarity_matrix)\n",
    "print(len(reconstruction))\n",
    "rank_with_sentence={}\n",
    "for i in range(len(sent_for_print)):\n",
    "  i = 0\n",
    "  if result_rank[i] not in set(rank_with_sentence.keys()): # error\n",
    "    rank_with_sentence[result_rank[i]]=[sent_for_print[i]]\n",
    "  else:\n",
    "    rank_with_sentence[result_rank[i]].append(sent_for_print[i])\n",
    "\n",
    "#sentences with their rank\n",
    "for key in sorted(rank_with_sentence.keys(),reverse=True)[:int(len(rank_with_sentence.keys())/4)]:\n",
    "  for k in rank_with_sentence[key]:\n",
    "    # print(key,k)\n",
    "    k[0].upper()\n",
    "    print(k) #### why??? kok gabisa di print\n",
    "    print(\"helaaaw?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
